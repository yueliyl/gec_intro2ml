{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yueliyl/gec_intro2ml/blob/A1-(WIP)/Tutorial_1_GEC_academy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Machine Learning: Assignment 1\n",
        "This tutorial will cover the basics of evaluating model performance. We will experiment with K-Nearest Neighbors (KNN) and Decision Tree (DT) models on the Iris benchmark dataset. The goal of this tutorial is to adapt the analysis to patient healthcare data used in the context of Assignment 1."
      ],
      "metadata": {
        "id": "xBgbNgucfGLi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This tutorial is an aggregation of three notebooks from the course repository @ https://github.com/yueliyl/gec_intro2ml/:\n",
        "\n",
        "\n",
        "*   [KNN.ipynb](https://github.com/yueliyl/gec_intro2ml/blob/master/KNN.ipynb)\n",
        "*   [DecisionTree.ipynb](https://github.com/yueliyl/gec_intro2ml/blob/master/DecisionTree.ipynb)\n",
        "*   [ModelEvaluationAndSelection.ipynb](https://github.com/yueliyl/gec_intro2ml/blob/master/ModelEvaluationAndSelection.ipynb)\n",
        "\n",
        "Feel free to consult these notebooks for additional material.\n",
        "\n"
      ],
      "metadata": {
        "id": "CwbugmcHDME7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Before running any code (if running on Google Colab)\n",
        "Create a copy of the notebook by going to **File -> Save a copy in Drive**. This notebook can be accessed via Google Colab by clicking on the badge below:\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mravanba/comp551-notebooks/blob/master/KNN.ipynb)"
      ],
      "metadata": {
        "id": "EAYolK1a3Wcy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup and data import"
      ],
      "metadata": {
        "id": "-XD88mE9tfpj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcUWbu3EIlnR"
      },
      "source": [
        "The first step in a Python analysis script is to import librairies. Here, we also set the random seed for reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4NB7743hndR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score, precision_recall_curve, auc\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "\n",
        "#the output of plotting commands is displayed inline within frontends\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "from IPython.core.debugger import set_trace         #for debugging\n",
        "\n",
        "#it is important to set the seed for reproducibility as it initializes the random number generator\n",
        "np.random.seed(1234)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrq-0LIMLnmD"
      },
      "source": [
        "##Data processing and visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rw721TwXhndY"
      },
      "source": [
        "We conveniently load the dataset from the sklearn collection of datasets. In this notebook we use the Iris dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phkF8ZRchndZ"
      },
      "outputs": [],
      "source": [
        "## to read more about load_iris() function refer to: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html\n",
        "data, classes = datasets.load_iris(return_X_y=True, as_frame=True)\n",
        "\n",
        "x = data.values[:,:2]\n",
        "y = classes.values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTIdYy7fhndc"
      },
      "source": [
        "We create the input matrix $X \\in \\mathbb{R}^{N \\times D}$ and the output vector $y \\in \\{1,\\ldots,C\\}^N$.\n",
        "Let's only use `sepal length` and `sepal width` for classification. We then randomly split the data into train and test and visualize the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWu7gTJWhndd",
        "tags": []
      },
      "outputs": [],
      "source": [
        "## dataset['data'] and dataset['target'] are both numpy arrays\n",
        "## dataset['data'] is (150, 4), and dataset['target'] is (150,)\n",
        "#x, y = dataset['data'][:,:2], dataset['target'] #slices the first two columns or features from the data\n",
        "\n",
        "## print the feature shape and classes of dataset\n",
        "(N,D), C = x.shape, np.max(y)+1\n",
        "print(f'instances (N) \\t {N} \\n features (D) \\t {D} \\n classes (C) \\t {C}')\n",
        "\n",
        "inds = np.random.permutation(N)      #generates an indices array from 0 to N-1 and permutes it\n",
        "\n",
        "## split the dataset into train and test\n",
        "x_train, y_train = x[inds[:100]], y[inds[:100]] # could alternatively use scikit-learn's train_test_split() function\n",
        "x_test, y_test = x[inds[100:]], y[inds[100:]]\n",
        "\n",
        "## visualization of the data\n",
        "fig, ax = plt.subplots(1,2,figsize=[10.,5.])\n",
        "\n",
        "class_index_to_name = {\n",
        "    0: 'Setosa',\n",
        "    1: 'Versicolor',\n",
        "    2: 'Virginica'\n",
        "}\n",
        "\n",
        "class_names = classes.map(class_index_to_name)\n",
        "\n",
        "# Get the first three colors from the 'viridis' colormap\n",
        "viridis_colors = plt.get_cmap('viridis').colors\n",
        "low_value_color = viridis_colors[0]\n",
        "middle_value_color = viridis_colors[len(viridis_colors) // 2]\n",
        "high_value_color = viridis_colors[-1]\n",
        "\n",
        "# Convert the colors to RGBA format\n",
        "low_value_rgba = mcolors.to_rgba(low_value_color)\n",
        "middle_value_rgba = mcolors.to_rgba(middle_value_color)\n",
        "high_value_rgba = mcolors.to_rgba(high_value_color)\n",
        "class_palette = {'Setosa': low_value_rgba, 'Versicolor': middle_value_rgba, 'Virginica': high_value_rgba}\n",
        "\n",
        "sns.scatterplot(data=data.iloc[:,:2], x='sepal length (cm)', y='sepal width (cm)', hue=class_names, palette=class_palette, ax=ax[0])\n",
        "ax[0].legend(title='classes (plant species)')\n",
        "\n",
        "ax[1].scatter(x_train[:,0], x_train[:,1], c=y_train, marker='o', label='train')\n",
        "ax[1].scatter(x_test[:,0], x_test[:,1], c=y_test, marker='s', label='test')\n",
        "ax[1].legend()\n",
        "ax[1].set_ylabel('sepal length (cm)')\n",
        "ax[1].set_xlabel('sepal width (cm)')\n",
        "\n",
        "## save figure to drive\n",
        "plt.savefig('iris.png', dpi=300)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6jPyDIRhndP"
      },
      "source": [
        "# K-Nearest Neighbours\n",
        "Our goal is to implement a K-NN classifier and apply it to classify the Iris dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The `KNN` class\n",
        "We implement our models as python classes. Two class methods that we usually need to implement are `fit` and `predict`, which respectively perform training by fitting the data and make prediction on new data. The model is initialized with the `__init__` function, and serves to initialise class attributes. Notably, such attributes include model *hyperparameters*."
      ],
      "metadata": {
        "id": "arnLKjR5laRx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#define the metric we will use to measure similarity\n",
        "#if the input shapes are [1,N1,F] and [N2,1,F] then output shape is [N2,N1]\n",
        "#as numpy supports broadcasting with arithmetic operations\n",
        "#for more on numpy broadcasting refer to: https://numpy.org/doc/stable/user/basics.broadcasting.html\n",
        "euclidean = lambda x1, x2: np.sqrt(np.sum((x1 - x2)**2, axis=-1))\n",
        "manhattan = lambda x1, x2: np.sum(np.abs(x1 - x2), axis=-1)\n",
        "\n",
        "class KNN:\n",
        "\n",
        "    def __init__(self, K=1, dist_fn= euclidean):\n",
        "        self.dist_fn = dist_fn\n",
        "        self.K = K\n",
        "        return\n",
        "\n",
        "    def fit(self, x, y):\n",
        "        ''' Store the training data using this method as it is a lazy learner'''\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.C = np.max(y) + 1\n",
        "        return self\n",
        "\n",
        "    def predict(self, x_test):\n",
        "        ''' Makes a prediction using the stored training data and the test data given as argument'''\n",
        "        num_test = x_test.shape[0]\n",
        "        # calculate distance between the training & test samples and returns an array of shape [num_test, num_train]\n",
        "        # self.x is in shape (100, 2), x_test is in shape (50, 2)\n",
        "        # self.x[None, :, :] is in shape (1, 100, 2), and x_test[:,None,:] is in shape (50, 1, 2)\n",
        "        # result: (x_test.shape[0], self.x.shape[0])\n",
        "        distances = self.dist_fn(self.x[None,:,:], x_test[:,None,:])\n",
        "        #ith-row of knns stores the indices of k closest training samples to the ith-test sample\n",
        "        knns = np.zeros((num_test, self.K), dtype=int)\n",
        "        #ith-row of y_prob has the probability distribution over C classes\n",
        "        y_prob = np.zeros((num_test, self.C))\n",
        "        for i in range(num_test):\n",
        "            # print(i)\n",
        "            knns[i,:] = np.argsort(distances[i])[:self.K]\n",
        "            # print(knns[i,:])\n",
        "            y_prob[i,:] = np.bincount(self.y[knns[i,:]], minlength=self.C) #counts the number of instances of each class in the K-closest training samples\n",
        "        #y_prob /= np.sum(y_prob, axis=-1, keepdims=True)\n",
        "        #simply divide by K to get a probability distribution\n",
        "        y_prob /= self.K\n",
        "        return y_prob, knns"
      ],
      "metadata": {
        "id": "eroKWs7klb8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuhuc942hndm"
      },
      "source": [
        "We next `fit` the model and make a prediction on test set using `predict`. We further connect each test node to its closest nearest neighbors in the plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gq3N8i02hndm",
        "tags": []
      },
      "outputs": [],
      "source": [
        "myK = 1\n",
        "\n",
        "model = KNN(K=myK)\n",
        "\n",
        "y_prob, knns = model.fit(x_train, y_train).predict(x_test)\n",
        "print('knns shape:', knns.shape)\n",
        "print('y_prob shape:', y_prob.shape)\n",
        "\n",
        "#To get hard predictions by choosing the class with the maximum probability\n",
        "y_pred = np.argmax(y_prob, axis=-1)\n",
        "accuracy = np.sum(y_pred == y_test)/y_test.shape[0]\n",
        "print(f'accuracy is {accuracy*100:.1f}.')\n",
        "\n",
        "#boolean array to later slice the indexes of correct and incorrect predictions\n",
        "correct = y_test == y_pred\n",
        "incorrect = np.logical_not(correct)\n",
        "\n",
        "#visualization of the points\n",
        "plt.scatter(x_train[:,0], x_train[:,1], c=y_train, marker='o', alpha=.2, label='train')\n",
        "plt.scatter(x_test[correct,0], x_test[correct,1], marker='.', c=y_pred[correct], label='correct')\n",
        "plt.scatter(x_test[incorrect,0], x_test[incorrect,1], marker='x', c=y_test[incorrect], label='misclassified')\n",
        "\n",
        "#connect each node to k-nearest neighbours in the training set\n",
        "for i in range(x_test.shape[0]):\n",
        "    for k in range(model.K):\n",
        "        hor = x_test[i,0], x_train[knns[i,k],0]\n",
        "        ver = x_test[i,1], x_train[knns[i,k],1]\n",
        "        plt.plot(hor, ver, 'k-', alpha=.1)\n",
        "\n",
        "plt.ylabel('sepal length')\n",
        "plt.xlabel('sepal width')\n",
        "plt.legend()\n",
        "plt.savefig('iris_KNN'+str(myK)+'.png',dpi=300,bbox_inches='tight')\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FR5qJ5LBhndq"
      },
      "source": [
        "## Decision Boundaries\n",
        "To draw the decision boundary we classify all the points on a 2D grid. The `meshgrid` function creates all the points on the grid by taking discretizations of horizontal and vertical axes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0udycx1Ohndq"
      },
      "outputs": [],
      "source": [
        "#we can make the grid finer by increasing the number of samples from 200 to higher value\n",
        "x0v = np.linspace(np.min(x[:,0]), np.max(x[:,0]), 200)\n",
        "x1v = np.linspace(np.min(x[:,1]), np.max(x[:,1]), 200)\n",
        "\n",
        "# to features values as a mesh\n",
        "x0, x1 = np.meshgrid(x0v, x1v)\n",
        "x_all = np.vstack((x0.ravel(),x1.ravel())).T\n",
        "\n",
        "for k in [1, 5, 15]:\n",
        "\n",
        "    model = KNN(K=k)\n",
        "\n",
        "    y_train_prob = np.zeros((y_train.shape[0], C))\n",
        "    y_train_prob[np.arange(y_train.shape[0]), y_train] = 1\n",
        "\n",
        "    y_prob, knns = model.fit(x_train, y_train).predict(x_test)\n",
        "    y_pred = np.argmax(y_prob,axis=-1)\n",
        "    y_pred_onehot  = pd.get_dummies(y_pred).values\n",
        "    y_train_onehot = pd.get_dummies(y_train).values\n",
        "\n",
        "    correct = y_test == y_pred\n",
        "    incorrect = np.logical_not(correct)\n",
        "\n",
        "    accuracy = np.sum(y_pred == y_test)/y_test.shape[0]\n",
        "    acc = f'accuracy: {accuracy*100:.1f}.'\n",
        "\n",
        "    ## to get class probability of all the points in the 2D grid\n",
        "    y_prob_all, _ = model.fit(x_train, y_train).predict(x_all)\n",
        "\n",
        "    y_pred_all = np.zeros_like(y_prob_all)\n",
        "    y_pred_all[np.arange(x_all.shape[0]), np.argmax(y_prob_all, axis=-1)] = 1\n",
        "\n",
        "    ## plot decision boundaries\n",
        "    fig, ax = plt.subplots(1,2,figsize=[10.,5.])\n",
        "\n",
        "    ## decision boundaries wrt final prediction\n",
        "    ax[0].set_title('final prediction')\n",
        "    ax[0].scatter(x_train[:,0], x_train[:,1], c=y_train_onehot, marker='o', alpha=1)\n",
        "    ax[0].scatter(x_all[:,0], x_all[:,1], c=y_pred_all, marker='.', alpha=.01)\n",
        "    ax[0].set_ylabel('sepal length')\n",
        "    ax[0].set_xlabel('sepal width')\n",
        "\n",
        "    ## decision boundaries wrt prediction probabilites\n",
        "    ax[1].set_title('prediction probabilities')\n",
        "    ax[1].scatter(x_train[:,0], x_train[:,1], c=y_train_prob, marker='o', alpha=1)\n",
        "    ax[1].scatter(x_all[:,0], x_all[:,1], c=y_prob_all, marker='.', alpha=.01)\n",
        "    ax[1].set_ylabel('sepal length')\n",
        "    ax[1].set_xlabel('sepal width')\n",
        "\n",
        "    fig.suptitle(f'K={k} -- {acc} %')\n",
        "    fig.tight_layout(); fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Choosing hyperparameter K in KNN"
      ],
      "metadata": {
        "id": "kN4KbkhmwBDK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We further split the training data into 50% training and 50% validation to choose the hyperparameter K."
      ],
      "metadata": {
        "id": "LRGvoNGYwEFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# further split the training data into 50% training and 50% validation\n",
        "x_train_tr, x_train_va, y_train_tr, y_train_va = \\\n",
        "  train_test_split(x_train, y_train, test_size=0.5, random_state=42)\n",
        "\n",
        "model_choices=[]\n",
        "train_accs, valid_accs = ([],[])\n",
        "\n",
        "n_valid = y_train_va.shape[0]\n",
        "model_choices = np.arange(1,11)\n",
        "\n",
        "for k in model_choices:\n",
        "\n",
        "    ## intialise KNN model\n",
        "    knn = KNN(K=k)\n",
        "\n",
        "    ## make predictions on training data\n",
        "    y_train_tr_prob,_ = knn.fit(x_train_tr, y_train_tr).predict(x_train_tr)\n",
        "    y_train_tr_pred = y_train_tr_prob.argmax(axis=-1)\n",
        "\n",
        "    ## make predictions on validation data\n",
        "    y_train_va_prob,_ = knn.predict(x_train_va)\n",
        "    y_train_va_pred = y_train_va_prob.argmax(axis=-1)\n",
        "\n",
        "    ## obtain training and validation classification accuracies\n",
        "    train_acc = np.sum(y_train_tr_pred == y_train_tr)/n_valid\n",
        "    valid_acc = np.sum(y_train_va_pred == y_train_va)/n_valid\n",
        "\n",
        "    ## store accuracies in lists for later use\n",
        "    train_accs.append(train_acc)\n",
        "    valid_accs.append(valid_acc)\n",
        "\n",
        "\n",
        "## find the best hyperparameter value K and initialise KNN model\n",
        "best_valid_K = model_choices[valid_accs.index(max(valid_accs))]\n",
        "knn = KNN(K=best_valid_K)\n",
        "\n",
        "## make prediction on testing data, only for best KNN model\n",
        "y_test_prob,_ = knn.fit(x_train, y_train).predict(x_test)\n",
        "y_test_pred = np.argmax(y_test_prob, axis=-1)\n",
        "\n",
        "## obtain testing accuracy\n",
        "test_accuracy = np.sum(y_test_pred == y_test)/y_test.shape[0]\n",
        "\n",
        "## plot training, validation and testing accuracies\n",
        "plt.plot(model_choices, train_accs, marker='o', color='green', label='training')\n",
        "plt.plot(model_choices, valid_accs, marker='o', color='blue', label='validation')\n",
        "plt.plot(best_valid_K, test_accuracy, marker='*', color='red', label='testing')\n",
        "\n",
        "plt.xlabel(\"K\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend(loc='best')\n",
        "plt.title(f'best K = {best_valid_K}, test accuracy = {test_accuracy}')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "16NqpdCiwHga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2D480I5NQLR"
      },
      "source": [
        "## KNN is sensitive to feature scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtI55uFPNQLS"
      },
      "outputs": [],
      "source": [
        "myK = 1\n",
        "\n",
        "model = KNN(K=myK)\n",
        "\n",
        "x_train_mod = x_train.copy()\n",
        "x_test_mod = x_test.copy()\n",
        "\n",
        "feature_id = 1 # feature to scale\n",
        "\n",
        "x_train_mod[:,feature_id] = x_train_mod[:,feature_id] * 100\n",
        "x_test_mod[:,feature_id] = x_test_mod[:,feature_id] * 100\n",
        "\n",
        "y_prob, knns = model.fit(x_train_mod, y_train).predict(x_test_mod)\n",
        "\n",
        "#To get hard predictions by choosing the class with the maximum probability\n",
        "y_pred = np.argmax(y_prob,axis=-1)\n",
        "accuracy = np.sum(y_pred == y_test)/y_test.shape[0]\n",
        "print(f'accuracy is {accuracy*100:.1f}.')\n",
        "\n",
        "#boolean array to later slice the indexes of correct and incorrect predictions\n",
        "correct = y_test == y_pred\n",
        "incorrect = np.logical_not(correct)\n",
        "\n",
        "#visualization of the points\n",
        "plt.scatter(x_train_mod[:,0], x_train_mod[:,1], c=y_train, marker='o', alpha=.2, label='train')\n",
        "plt.scatter(x_test_mod[correct,0], x_test_mod[correct,1], marker='.', c=y_pred[correct], label='correct')\n",
        "plt.scatter(x_test_mod[incorrect,0], x_test_mod[incorrect,1], marker='x', c=y_test[incorrect], label='misclassified')\n",
        "\n",
        "#connect each node to k-nearest neighbours in the training set\n",
        "for i in range(x_test_mod.shape[0]):\n",
        "    for k in range(model.K):\n",
        "        hor = x_test_mod[i,0], x_train_mod[knns[i,k],0]\n",
        "        ver = x_test_mod[i,1], x_train_mod[knns[i,k],1]\n",
        "        plt.plot(hor, ver, 'k-', alpha=.1)\n",
        "\n",
        "plt.ylabel('sepal length')\n",
        "plt.xlabel('sepal width')\n",
        "plt.legend()\n",
        "plt.savefig('iris_KNN'+str(myK)+'_scaledX'+str(feature_id)+'.png',dpi=300, bbox_inches='tight')\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtmHY41cNQLS"
      },
      "source": [
        "## Standardizing features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-auB98QNQLT"
      },
      "outputs": [],
      "source": [
        "model = KNN(K=5)\n",
        "\n",
        "x_train_mod = x_train.copy()\n",
        "x_test_mod = x_test.copy()\n",
        "x_mod = x.copy()\n",
        "\n",
        "x_train_mod[:,0] = x_train_mod[:,0] * 10\n",
        "x_test_mod[:,0]  = x_test_mod[:,0] * 10\n",
        "x_mod[:,0] = x_mod[:,0] * 10\n",
        "\n",
        "#we can make the grid finer by increasing the number of samples from 200 to higher value\n",
        "x0v = np.linspace(np.min(x[:,0]), np.max(x[:,0]), 200)\n",
        "x1v = np.linspace(np.min(x[:,1]), np.max(x[:,1]), 200)\n",
        "x0, x1 = np.meshgrid(x0v, x1v)\n",
        "x_all = np.vstack((x0.ravel(),x1.ravel())).T\n",
        "\n",
        "x0v_mod = np.linspace(np.min(x_mod[:,0]), np.max(x_mod[:,0]), 200)\n",
        "x1v_mod = np.linspace(np.min(x_mod[:,1]), np.max(x_mod[:,1]), 200)\n",
        "x0_mod, x1_mod = np.meshgrid(x0v_mod, x1v_mod)\n",
        "x_all_mod = np.vstack((x0_mod.ravel(),x1_mod.ravel())).T\n",
        "\n",
        "## make predictions on both original features and standardized features\n",
        "y_prob    , _ = model.fit(x_train    , y_train).predict(x_test)\n",
        "y_prob_mod, _ = model.fit(x_train_mod, y_train).predict(x_test_mod)\n",
        "y_prob_all    , _ = model.fit(x_train,     y_train).predict(x_all)\n",
        "y_prob_all_mod, _ = model.fit(x_train_mod, y_train).predict(x_all_mod)\n",
        "\n",
        "#To get hard predictions by choosing the class with the maximum probability\n",
        "y_pred     = y_prob.argmax(-1)\n",
        "y_pred_mod = y_prob_mod.argmax(-1)\n",
        "y_pred_all = y_prob_all.argmax(-1)\n",
        "y_pred_all_mod = y_prob_all_mod.argmax(-1)\n",
        "\n",
        "y_pred_onehot     = pd.get_dummies(y_pred).values\n",
        "y_pred_mod_onehot = pd.get_dummies(y_pred_mod).values\n",
        "y_pred_all_onehot = pd.get_dummies(y_pred_all).values\n",
        "y_pred_all_onehot_mod = pd.get_dummies(y_pred_all_mod).values\n",
        "y_train_ohehot    = pd.get_dummies(y_train).values\n",
        "\n",
        "accuracy     = np.sum(y_pred     == y_test)/y_test.shape[0]\n",
        "accuracy_mod = np.sum(y_pred_mod == y_test)/y_test.shape[0]\n",
        "#print(f'accuracy is {accuracy*100:.1f}.')\n",
        "\n",
        "fig, ax = plt.subplots(1,3,figsize=[15.,5.])\n",
        "\n",
        "##\n",
        "ax[0].set_title('original features')\n",
        "ax[0].scatter(x_train[:,0], x_train[:,1], c=y_train_onehot, marker='o', alpha=1)\n",
        "ax[0].scatter(x_all[:,0], x_all[:,1], c=y_pred_all_onehot, marker='.', alpha=.01)\n",
        "ax[0].set_ylabel('sepal length')\n",
        "ax[0].set_xlabel('sepal width')\n",
        "\n",
        "##\n",
        "ax[1].set_title('modified features')\n",
        "ax[1].scatter(x_train_mod[:,0], x_train_mod[:,1], c=y_train_onehot, marker='o', alpha=1)\n",
        "ax[1].scatter(x_all_mod[:,0], x_all_mod[:,1], c=y_pred_all_onehot_mod, marker='.', alpha=.01)\n",
        "ax[1].set_ylabel('sepal length')\n",
        "ax[1].set_xlabel('sepal width')\n",
        "\n",
        "plt.legend()\n",
        "# plt.show()\n",
        "\n",
        "for d in range(x_train.shape[1]):\n",
        "    x_train_mod[:,d] = (x_train_mod[:,d] - x_train_mod[:,d].mean())/x_train_mod[:,d].std()\n",
        "    x_test_mod[:,d] = (x_test_mod[:,d] - x_test_mod[:,d].mean())/x_test_mod[:,d].std()\n",
        "    x_mod[:,d] = (x_mod[:,d] - x_mod[:,d].mean())/x_mod[:,d].std()\n",
        "\n",
        "x0v_mod = np.linspace(np.min(x_mod[:,0]), np.max(x_mod[:,0]), 200)\n",
        "x1v_mod = np.linspace(np.min(x_mod[:,1]), np.max(x_mod[:,1]), 200)\n",
        "x0_mod, x1_mod = np.meshgrid(x0v_mod, x1v_mod)\n",
        "x_all_mod = np.vstack((x0_mod.ravel(),x1_mod.ravel())).T\n",
        "\n",
        "## make predictions on both original features and standardized features\n",
        "y_prob_mod, _ = model.fit(x_train_mod, y_train).predict(x_test_mod)\n",
        "y_prob_all_mod, _ = model.fit(x_train_mod, y_train).predict(x_all_mod)\n",
        "\n",
        "#To get hard predictions by choosing the class with the maximum probability\n",
        "y_pred_mod = y_prob_mod.argmax(-1)\n",
        "y_pred_all_mod = y_prob_all_mod.argmax(-1)\n",
        "\n",
        "y_pred_mod_onehot = pd.get_dummies(y_pred_mod).values\n",
        "y_pred_all_onehot_mod = pd.get_dummies(y_pred_all_mod).values\n",
        "\n",
        "accuracy_mod = np.sum(y_pred_mod == y_test)/y_test.shape[0]\n",
        "\n",
        "ax[2].set_title('standardize feature')\n",
        "ax[2].scatter(x_train_mod[:,0], x_train_mod[:,1], c=y_train_onehot, marker='o', alpha=1)\n",
        "ax[2].scatter(x_all_mod[:,0], x_all_mod[:,1], c=y_pred_all_onehot_mod, marker='.', alpha=.01)\n",
        "ax[2].set_ylabel('sepal length')\n",
        "ax[2].set_xlabel('sepal width')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yy1mPclFhnd0"
      },
      "source": [
        "## The Effect of Noise and Feature Scaling\n",
        "Nearest neighbours are highly affected by the existence of noisy or irrelevant features.\n",
        "Lets add noise and see how it affects the accuracy of our classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQn3SBaNhnd1"
      },
      "outputs": [],
      "source": [
        "noise_scale = [.01, .1, 1, 10, 100, 1000]\n",
        "\n",
        "## generate random noise\n",
        "noise = np.random.randn(x.shape[0],1)\n",
        "results = []\n",
        "for s in noise_scale:\n",
        "    ## add noise as an extra feature\n",
        "    x_n = np.column_stack((x, noise*s))\n",
        "    results.append([])\n",
        "\n",
        "    ##repeat the experiment 100 times with different train and test split\n",
        "    for r in range(100):\n",
        "        ## data processing and train-test split\n",
        "        inds = np.random.permutation(N)\n",
        "        x_train, y_train = x_n[inds[:100]], y[inds[:100]]\n",
        "        x_test, y_test = x_n[inds[100:]], y[inds[100:]]\n",
        "\n",
        "        ## define model\n",
        "        model = KNN(K=3)\n",
        "\n",
        "        ## prediction\n",
        "        y_prob, _ = model.fit(x_train, y_train).predict(x_test)\n",
        "        y_pred = np.argmax(y_prob,1)\n",
        "        accuracy = np.sum(y_pred == y_test)/y_test.shape[0]\n",
        "        results[-1].append(accuracy)\n",
        "\n",
        "results = np.array(results)\n",
        "plt.errorbar(noise_scale, results.mean(1), results.std(1))\n",
        "plt.xscale('log')\n",
        "plt.xlabel(\"scale of the noisy feature\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sLeEWDBhnd4"
      },
      "source": [
        "As we increase the noise level of spurious features, these features dominate distance calculattions such that the K nearest neighbors are more affected by the spurious features rather than the relevant ones."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree (DT)\n",
        "We implement a decision tree for classification from scratch. We first define a data-structure to represent nodes, then we define a function to pick the best *test* for a node based on a *cost function*. Starting from the root node, we use this test function recursively to find the best split until a *max-depth* is reached."
      ],
      "metadata": {
        "id": "hZ9T4xqPn8oT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-9QNkez92Ub",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Greedy spliting of a node\n",
        "Given a node, we consider all possible feature-value combinations for splitting the data. One such **test** that produces the lowest cost according to a supplied cost function (`cost_fn`) is returned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJclNLqw92Ub",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def greedy_test(node, cost_fn):\n",
        "    ## initialize the best parameter values\n",
        "    best_cost = np.inf\n",
        "    best_feature, best_value = None, None\n",
        "    num_instances, num_features = node.data.shape\n",
        "\n",
        "    ## sort the features to get the test value candidates by taking the average of consecutive sorted feature values\n",
        "    data_sorted = np.sort(node.data[node.data_indices],axis=0)\n",
        "    test_candidates = (data_sorted[1:] + data_sorted[:-1]) / 2.\n",
        "    for f in range(num_features):\n",
        "        ##stores the data corresponding to the f-th feature\n",
        "        data_f = node.data[node.data_indices, f]\n",
        "\n",
        "        for test in test_candidates[:,f]:\n",
        "            ## Split the indices using the test value of f-th feature\n",
        "            left_indices = node.data_indices[data_f <= test]\n",
        "            right_indices = node.data_indices[data_f > test]\n",
        "            #we can't have a split where a child has zero element\n",
        "            #if this is true over all the test features and their test values  then the function returns the best cost as infinity\n",
        "            if len(left_indices) == 0 or len(right_indices) == 0:\n",
        "                continue\n",
        "\n",
        "            ## compute the left and right cost based on the current split\n",
        "            left_cost = cost_fn(node.labels[left_indices])\n",
        "            right_cost = cost_fn(node.labels[right_indices])\n",
        "            num_left, num_right = left_indices.shape[0], right_indices.shape[0]\n",
        "\n",
        "            ## get the combined cost using the weighted sum of left and right cost\n",
        "            cost = (num_left * left_cost + num_right * right_cost)/num_instances\n",
        "\n",
        "            ## update only when a lower cost is encountered\n",
        "            if cost < best_cost:\n",
        "                best_cost = cost\n",
        "                best_feature = f\n",
        "                best_value = test\n",
        "\n",
        "    return best_cost, best_feature, best_value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_z4Qs8092UY",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Data structure\n",
        "We use a binary tree to assign information to the nodes of the decision tree. In particular, the instances associated with that node (`data_indices`) and the test that is used to further split that node, in the case non-leaf nodes (`split_feature, split_value`). Here, we assume each test involves a single feature, and all features are real-valued."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEwmX3Xi92UY",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "class Node:\n",
        "    def __init__(self, data_indices, parent):\n",
        "        self.data_indices = data_indices                    #stores the data indices which are in the region defined by this node\n",
        "        self.left = None                                    #stores the left child of the node\n",
        "        self.right = None                                   #stores the right child of the node\n",
        "        self.split_feature = None                           #the feature for split at this node\n",
        "        self.split_value = None                             #the value of the feature for split at this node\n",
        "        if parent:\n",
        "            self.depth = parent.depth + 1                   #obtain the dept of the node by adding one to dept of the parent\n",
        "            self.num_classes = parent.num_classes           #copies the num classes from the parent\n",
        "            self.data = parent.data                         #copies the data from the parent\n",
        "            self.labels = parent.labels                     #copies the labels from the parent\n",
        "            class_prob = np.bincount(self.labels[data_indices], minlength=self.num_classes) #this is counting frequency of different labels in the region defined by this node\n",
        "            self.class_prob = class_prob / np.sum(class_prob)  #stores the class probability for the node\n",
        "            #note that we'll use the class probabilites of the leaf nodes for making predictions after the tree is built"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GjjJhkb92Ue",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Cost functions\n",
        "\n",
        "Below we implement two cost functions corresponding to misclassification rate and entropy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Fa_FkuK92Uf",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "## computes misclassification cost by subtracting the maximum probability of any class\n",
        "def cost_misclassification(labels):\n",
        "    counts = np.bincount(labels)\n",
        "    class_probs = counts / np.sum(counts)\n",
        "    return 1 - np.max(class_probs)\n",
        "\n",
        "## computes entropy of the labels by computing the class probabilities\n",
        "def cost_entropy(labels):\n",
        "    class_probs = np.bincount(labels) / len(labels)\n",
        "    class_probs = class_probs[class_probs > 0]              #this steps is remove 0 probabilities for removing numerical issues while computing log\n",
        "    return -np.sum(class_probs * np.log2(class_probs))       #expression for entropy -\\sigma p(x)log[p(x)]\n",
        "\n",
        "## computes the gini index cost\n",
        "def cost_gini_index(labels):\n",
        "    class_probs = np.bincount(labels) / len(labels)\n",
        "    return 1 - np.sum(np.square(class_probs))               #expression for gini index 1-\\sigma p(x)^2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparison of the 3 cost functions for binary class\n",
        "\n",
        "Entropy and Gini index behave similarly."
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "JlI3r6Igf52P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "x = np.linspace(0.01, 0.99, 100)\n",
        "f1 = 1 - np.max(np.stack((x, 1-x)),axis=0)\n",
        "f2 = -x * np.log2(x) - (1-x) * np.log2(1-x)\n",
        "f3 = 2 * x * (1 - x)\n",
        "\n",
        "plt.clf()\n",
        "plt.figure(figsize=(4.8, 3.6))\n",
        "plt.plot(x, f1, \"-\", label=\"Misclassification\")\n",
        "plt.plot(x, f2, \"-\", label=\"Entropy\")\n",
        "plt.plot(x, f3, \"--\", label=\"Gini Index\")\n",
        "plt.xlabel(\"$p(y=1)$\")\n",
        "plt.ylabel(\"cost\")\n",
        "plt.legend()"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "2zB1I9kYf52P"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDlnrs8U92Uh",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "we are now ready to implement the classifier:\n",
        "\n",
        "when to stop splitting?\n",
        "1. if there are not enough points under the current node (`min_leaf_instances`)\n",
        "2. if maximum depth has been reached (`max_depth`)\n",
        "3. if the decrease in the cost is negligible (not implemented)\n",
        "\n",
        "Below is a template of the `DecisionTree` class. We next implement `fit` and `predict` methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMx7Brw292Uh",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "class DecisionTree:\n",
        "    def __init__(self, num_classes=None, max_depth=3, cost_fn=cost_misclassification, min_leaf_instances=1):\n",
        "        self.max_depth = max_depth      #maximum dept for termination\n",
        "        self.root = None                #stores the root of the decision tree\n",
        "        self.cost_fn = cost_fn          #stores the cost function of the decision tree\n",
        "        self.num_classes = num_classes  #stores the total number of classes\n",
        "        self.min_leaf_instances = min_leaf_instances  #minimum number of instances in a leaf for termination\n",
        "\n",
        "    def fit(self, data, labels):\n",
        "        pass                            #pass in python 3 means nothing happens and the method here is empty\n",
        "\n",
        "    def predict(self, data_test):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gbsp5yvB92Ul",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Fit\n",
        "To fit the tree to the data, we call the `_fit_tree` method on the `root_node` of the tree. In this method the _best_ test (split) for the current node is found, and the method is recursively called on the left and right child. If the `max_depth` is reached or the number of instances under the current node is below `min_leaf_instances` the node is not split anymore, resulting in a leaf node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NOsuJEc92Ul",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def fit(self, data, labels):\n",
        "    self.data = data\n",
        "    self.labels = labels\n",
        "    if self.num_classes is None:\n",
        "        self.num_classes = np.max(labels) + 1\n",
        "\n",
        "    ## below are initialization of the root of the decision tree\n",
        "    self.root = Node(np.arange(data.shape[0]), None)\n",
        "    self.root.data = data\n",
        "    self.root.labels = labels\n",
        "    self.root.num_classes = self.num_classes\n",
        "    self.root.depth = 0\n",
        "\n",
        "    ## to recursively build the rest of the tree\n",
        "    self._fit_tree(self.root)\n",
        "    return self\n",
        "\n",
        "def _fit_tree(self, node):\n",
        "    ## This gives the condition for termination of the recursion resulting in a leaf node\n",
        "    if node.depth == self.max_depth or len(node.data_indices) <= self.min_leaf_instances:\n",
        "        return\n",
        "\n",
        "    ## greedily select the best test by minimizing the cost\n",
        "    cost, split_feature, split_value = greedy_test(node, self.cost_fn)\n",
        "\n",
        "    ## if the cost returned is infinity it means that it is not possible to split the node and hence terminate\n",
        "    if np.isinf(cost):\n",
        "        return\n",
        "    #print(f'best feature: {split_feature}, value {split_value}, cost {cost}')\n",
        "\n",
        "    ## to get a boolean array suggesting which data indices corresponding to this node are in the left of the split\n",
        "    test = node.data[node.data_indices,split_feature] <= split_value\n",
        "\n",
        "    ## store the split feature and value of the node\n",
        "    node.split_feature = split_feature\n",
        "    node.split_value = split_value\n",
        "\n",
        "    ##define new nodes which are going to be the left and right child of the present node\n",
        "    left = Node(node.data_indices[test], node)\n",
        "    right = Node(node.data_indices[np.logical_not(test)], node)\n",
        "\n",
        "    ## recursive call to the _fit_tree()\n",
        "    self._fit_tree(left)\n",
        "    self._fit_tree(right)\n",
        "\n",
        "    ## assign the left and right child to present child\n",
        "    node.left = left\n",
        "    node.right = right\n",
        "\n",
        "## assign class fitting methods\n",
        "DecisionTree.fit = fit\n",
        "DecisionTree._fit_tree = _fit_tree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAZdSNN392Ut",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## New predictions\n",
        "Given a new instance, we start from the `root` of the `DecisionTree` and check whether the instance falls in the left or right split. The process is repeated at the left or right node depending on the test, until a leaf node is reached. The class probabilities of the training data under the leaf is returned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLmilufw92Uu",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def predict(self, data_test):\n",
        "    class_probs = np.zeros((data_test.shape[0], self.num_classes))\n",
        "    for n, x in enumerate(data_test):\n",
        "        node = self.root\n",
        "\n",
        "        ## loop along the dept of the tree looking region where the present data sample fall in based on the split feature and value\n",
        "        while node.left:\n",
        "            if x[node.split_feature] <= node.split_value:\n",
        "                node = node.left\n",
        "            else:\n",
        "                node = node.right\n",
        "\n",
        "        ## the loop terminates when you reach a leaf of the tree and the class probability of that node is taken for prediction\n",
        "        class_probs[n,:] = node.class_prob\n",
        "\n",
        "    return class_probs\n",
        "\n",
        "## assign class prediction method\n",
        "DecisionTree.predict = predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1Eoy0VX92Ux",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Experiments\n",
        "We will experiment on the same Iris dataset as we did when exploring the KNN model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTRlBSMj92U1",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "dataset = datasets.load_iris()\n",
        "x, y = dataset['data'][:,:2], dataset['target']\n",
        "(num_instances, num_features), num_classes = x.shape, np.max(y)+1\n",
        "\n",
        "inds = np.random.permutation(num_instances)\n",
        "x_train, y_train = x[inds[:100]], y[inds[:100]]\n",
        "x_test, y_test = x[inds[100:]], y[inds[100:]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aJbD0MD92U8",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "We now fit the `DecisionTree` on training data and predict on testing data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpzdRsap92U9",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "tree = DecisionTree(max_depth=20)\n",
        "probs_test = tree.fit(x_train, y_train).predict(x_test)\n",
        "y_pred = np.argmax(probs_test,1)\n",
        "accuracy = np.sum(y_pred == y_test)/y_test.shape[0]\n",
        "print(f'accuracy is {accuracy*100:.1f}.')\n",
        "\n",
        "## visualize classifications\n",
        "correct = y_test == y_pred\n",
        "incorrect = np.logical_not(correct)\n",
        "plt.scatter(x_train[:,0], x_train[:,1], c=y_train, marker='o', alpha=.2, label='train')\n",
        "plt.scatter(x_test[correct,0], x_test[correct,1], marker='.', c=y_pred[correct], label='correct')\n",
        "plt.scatter(x_test[incorrect,0], x_test[incorrect,1], marker='x', c=y_test[incorrect], label='misclassified')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZx26Ndk92VE",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "We now plot the decision boundary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "## create 2D grid\n",
        "x0v = np.linspace(np.min(x[:,0]), np.max(x[:,0]), 200)\n",
        "x1v = np.linspace(np.min(x[:,1]), np.max(x[:,1]), 200)\n",
        "x0,x1 = np.meshgrid(x0v, x1v)\n",
        "x_all = np.vstack((x0.ravel(),x1.ravel())).T\n",
        "\n",
        "y_train_onehot = pd.get_dummies(y_train).values\n",
        "\n",
        "## loop through different tree depths (hyperparameter)\n",
        "for tree_depth in [1,3,7,10]:\n",
        "\n",
        "    ## initialise DT model\n",
        "    model = DecisionTree(max_depth=tree_depth)\n",
        "\n",
        "    ## make prediction\n",
        "    y_prob_all = model.fit(x_train, y_train).predict(x_all)\n",
        "    y_pred_all = y_prob_all.argmax(-1)\n",
        "\n",
        "    ## one-hot encode, ensuring that all classes are included in the encoding\n",
        "    y_pred_all_onehot = pd.get_dummies(y_pred_all)\n",
        "    classes = np.arange(y.max()+1)\n",
        "    class_not_in_columns = np.where(~np.isin(classes, y_pred_all_onehot.columns.tolist()))[0]\n",
        "    y_pred_all_onehot[class_not_in_columns] = 0\n",
        "    y_pred_all_onehot.reindex(columns=classes)\n",
        "    y_pred_all_onehot = y_pred_all_onehot.values\n",
        "\n",
        "    fig, ax = plt.subplots(1,2,figsize=[10.,5.])\n",
        "\n",
        "    ax[0].set_title('final prediction')\n",
        "    ax[0].scatter(x_train[:,0], x_train[:,1], c=y_train_onehot, marker='o', alpha=1)\n",
        "    ax[0].scatter(x_all[:,0], x_all[:,1], c=y_pred_all_onehot, marker='.', alpha=.02)\n",
        "    ax[0].set_ylabel('sepal length')\n",
        "    ax[0].set_xlabel('sepal width')\n",
        "\n",
        "    ax[1].set_title('prediction probabilities')\n",
        "    ax[1].scatter(x_train[:,0], x_train[:,1], c=y_train_onehot, marker='o', alpha=1)\n",
        "    ax[1].scatter(x_all[:,0], x_all[:,1], c=y_prob_all, marker='.', alpha=.02)\n",
        "    ax[1].set_ylabel('sepal length')\n",
        "    ax[1].set_xlabel('sepal width')\n",
        "\n",
        "    fig.suptitle('Depth = ' + str(tree_depth))\n",
        "    fig.show()"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "-crjx5U8f52U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# further split the training data into 50% training and 50% validation\n",
        "X_train_tr, y_train_tr = x_train[:50], y_train[:50]\n",
        "X_train_va, y_train_va = x_train[50:], y_train[50:]\n",
        "\n",
        "model_choices=[]\n",
        "train_acc = []\n",
        "valid_acc = []\n",
        "\n",
        "n_train = y_train_tr.shape[0]\n",
        "n_valid = y_train_va.shape[0]\n",
        "\n",
        "for k in range(1, 11):\n",
        "    dt = DecisionTree(max_depth=k) # create a KNN object (OOP)\n",
        "\n",
        "    y_train_tr_prob = dt.fit(X_train_tr, y_train_tr).predict(X_train_tr)\n",
        "    y_train_tr_pred = np.argmax(y_train_tr_prob, axis=-1)\n",
        "    acc_tr = np.sum(y_train_tr_pred == y_train_tr)/n_train\n",
        "\n",
        "    y_train_va_prob = dt.fit(X_train_tr, y_train_tr).predict(X_train_va)\n",
        "    y_train_va_pred = np.argmax(y_train_va_prob, axis=-1)\n",
        "    acc_va = np.sum(y_train_va_pred == y_train_va)/n_valid\n",
        "\n",
        "    model_choices.append(k)\n",
        "    train_acc.append(acc_tr)\n",
        "    valid_acc.append(acc_va)\n",
        "\n",
        "# use the best K to predict test data\n",
        "best_depth = model_choices[valid_acc.index(max(valid_acc))]\n",
        "dt = DecisionTree(max_depth=best_depth)\n",
        "y_test_prob = dt.fit(x_train, y_train).predict(x_test)\n",
        "y_test_pred = np.argmax(y_test_prob, axis=-1)\n",
        "test_accuracy = np.sum(y_test_pred == y_test)/y_test.shape[0]\n",
        "\n",
        "plt.plot(model_choices, train_acc, marker='d', color='green', label='training')\n",
        "plt.plot(model_choices, valid_acc, marker='o', color='blue', label='validation')\n",
        "plt.plot(best_depth, test_accuracy, marker='*', color='red', label='testing')\n",
        "\n",
        "plt.title(f'best depth = {best_depth}, test accuracy = {test_accuracy}')\n",
        "plt.xlabel(\"Tree depth\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "bbQ0GKipf52U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation and Selection\n",
        "\n",
        "We will continue working with the Iris dataset. However, we will only retain two of the three classes (i.e. plant species) for the following analyses. Having only two classes allows to treat our task as a binary classification problem. Specifically, one class will represent \"positive\" instances whereas the other class will represent \"negative\" instances.\n",
        "\n",
        "This setup might not make complete sense in the context of binary classification, meaning there is no reason to treat one plant species as \"positive\" and the other species as \"negative\". However, our analyses will be adaptable to the datasets used in Assignment 1, where target classes have a clear qualitative difference. For instance:\n",
        "\n",
        "\n",
        "*   Survive vs Die\n",
        "*   Signs of diabetic retinopathy vs NO signs of diabetic retinopathy\n",
        "\n"
      ],
      "metadata": {
        "id": "v9HWhQmF616K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## True/false positive rates\n",
        "With \"positive\" and \"negative\" classes, we can analyse classification results by considering the following categories:\n",
        "\n",
        "True negative (TN)\n",
        "* Negative example that is predicted to be negative\n",
        "\n",
        "False positive (FP)\n",
        "* Negative example that is predicted to be positive\n",
        "\n",
        "False negative (FN)\n",
        "* Positive example that is predicted to be negative\n",
        "\n",
        "True positive (TP)\n",
        "* Positive example that is predicted to be positive"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "uaarPXJmfgPE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Receiver Operating Characteristic (ROC) Curve\n",
        "\n",
        "Instead of setting an arbitrary threhold on the probabilities score we can evalaute the method based on *all thresholds* and compute the ROC using the resulting True Postiive Rates (y-axis) and False Positive Rates (x-axis) as follows."
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "LK9s7USwfgPF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data, classes = datasets.load_iris(return_X_y=True, as_frame=True)\n",
        "\n",
        "## only keep two of three plant species (Setosa and Versicolour)\n",
        "keep = np.isin(classes.values, [1,2])\n",
        "classes = classes.loc[keep]\n",
        "\n",
        "x = data[keep].iloc[:,:2]\n",
        "\n",
        "class_index_to_name = {\n",
        "    0: 'Setosa',\n",
        "    1: 'Versicolor - neg. class',\n",
        "    2: 'Virginica - pos. class'\n",
        "}\n",
        "\n",
        "class_names = classes.map(class_index_to_name)\n",
        "\n",
        "# Get the first three colors from the 'viridis' colormap\n",
        "viridis_colors = plt.get_cmap('viridis').colors\n",
        "low_value_color = viridis_colors[0]\n",
        "middle_value_color = viridis_colors[len(viridis_colors) // 2]\n",
        "high_value_color = viridis_colors[-1]\n",
        "\n",
        "# Convert the colors to RGBA format\n",
        "low_value_rgba = mcolors.to_rgba(low_value_color)\n",
        "middle_value_rgba = mcolors.to_rgba(middle_value_color)\n",
        "high_value_rgba = mcolors.to_rgba(high_value_color)\n",
        "\n",
        "class_palette = {'Setosa': low_value_rgba, 'Versicolor - neg. class': middle_value_rgba, 'Virginica - pos. class': high_value_color}\n",
        "\n",
        "sns.scatterplot(data=x, x='sepal length (cm)', y='sepal width (cm)', hue=class_names, palette=class_palette)\n",
        "plt.legend(title='classes (flower species)')\n",
        "\n",
        "x = x.values\n",
        "y = classes.values\n",
        "\n",
        "## ensure that classes labelled as 0/1\n",
        "y[y == y.min()] = 0\n",
        "y[y == y.max()] = 1"
      ],
      "metadata": {
        "id": "jOXDaO7UBHvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can compute a confusion matrix that contains TN, FP, FN, and TP as follows."
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "mBDhIG21fgPF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## create training and testing datasets. then, from the training set, create training set and validation set\n",
        "x_train, x_test, y_train, y_test = \\\n",
        "    train_test_split(x, y, test_size=0.3, random_state=42)\n",
        "\n",
        "x_train, x_valid, y_train, y_valid = \\\n",
        "    train_test_split(x_train, y_train, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "DLGNwvHwWHA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "knn = KNN(K=3)\n",
        "dt  = DecisionTree(max_depth=2)\n",
        "\n",
        "y_test_prob_knn, _ = knn.fit(x_train, y_train).predict(x_test)\n",
        "y_test_prob_dt     = dt.fit(x_train, y_train).predict(x_test)\n",
        "\n",
        "y_test_pred_knn = y_test_prob_knn.argmax(-1)\n",
        "y_test_pred_dt  = y_test_prob_dt.argmax(-1)"
      ],
      "metadata": {
        "id": "kMVzUTdgCVM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "### confusion matrix ###\n",
        "\n",
        "cm_knn = confusion_matrix(y_test, y_test_pred_knn)\n",
        "cm_dt  = confusion_matrix(y_test, y_test_pred_dt)\n",
        "\n",
        "cm_df_knn = pd.DataFrame(cm_knn, index=['Actual Negative (Versicolor)','Actual Positive (Virginica)'],\n",
        "                     columns=['Predicted Negative', 'Predicted Positive'])\n",
        "\n",
        "cm_df_dt = pd.DataFrame(cm_dt, index=['Actual Negative (Versicolor)','Actual Positive (Virginica)'],\n",
        "                     columns=['Predicted Negative', 'Predicted Positive'])\n",
        "\n",
        "print('KNN confusion matrix')\n",
        "print(cm_df_knn)\n",
        "print()\n",
        "print('DT confusion matrix')\n",
        "print(cm_df_dt)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "aBOtTYnbfgPF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember: True-Positive Rate (TPR) and False-Positive Rate (FPR)\n",
        "\n",
        "$TPR = \\frac{TP}{TP + FN}$\n",
        "\n",
        "and\n",
        "\n",
        "$FPR = \\frac{FP}{FP + TN}$"
      ],
      "metadata": {
        "id": "7OTghrT3A-If"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "## Receiver-Operator Curve (ROC)\n",
        "\n",
        "y_test_prob_dt = dt.predict(x_test)\n",
        "y_test_prob_dt = y_test_prob_dt[:,1]\n",
        "\n",
        "y_test_prob_knn, _ = knn.predict(x_test)\n",
        "y_test_prob_knn = y_test_prob_knn[:,1]\n",
        "\n",
        "fpr_dt, tpr_dt, thresholds_dt = roc_curve(y_test, y_test_prob_dt)\n",
        "roc_auc_dt = roc_auc_score(y_test, y_test_prob_dt)\n",
        "\n",
        "fpr_knn, tpr_knn, thresholds_knn = roc_curve(y_test, y_test_prob_knn)\n",
        "roc_auc_knn = roc_auc_score(y_test, y_test_prob_knn)\n",
        "\n",
        "plt.clf()\n",
        "\n",
        "plt.plot(fpr_dt, tpr_dt, \"b-\", lw=2, label=\"DT (AUROC = %0.2f)\"%roc_auc_dt)\n",
        "plt.plot(fpr_knn, tpr_knn, \"g-\", lw=2, label=\"KNN (AUROC = %0.2f)\"%roc_auc_knn)\n",
        "\n",
        "plt.axline((0, 0), (1, 1), linestyle=\"--\", lw=1, color='gray')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC in predicting Versicolor')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "jsK8owkkfgPF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cross validation\n",
        "\n",
        "In our above example, we split the data into training and testing set. This is quite wasteful because we sacrifice half of the data. To evaluate the data on *every data point*, we can perform K-fold cross-validation.\n",
        "\n",
        "<img src=\"https://github.com/yueliyl/comp551-notebooks/blob/master/kfold_cv/cross_validation.png?raw=1\" width=1200>\n",
        "\n",
        "<img src=\"https://github.com/yueliyl/comp551-notebooks/blob/master/kfold_cv/cross_validation_2.png?raw=1\" width=1000>"
      ],
      "metadata": {
        "id": "vN7RLee-1Iia"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "def cross_validate(model, X_input, Y_output):\n",
        "    kf = KFold(n_splits=5, random_state=1, shuffle=True)\n",
        "    y = np.array([0] * X_input.shape[0])\n",
        "    yh = np.array([0.0] * X_input.shape[0])\n",
        "\n",
        "    for train_index, test_index in kf.split(X_input):\n",
        "        model.fit(X_input[train_index], Y_output[train_index])\n",
        "        y[test_index] = Y_output[test_index]\n",
        "\n",
        "        tmp = model.predict(X_input[test_index])\n",
        "\n",
        "        if len(tmp) == 2:\n",
        "          yh_tmp, _ = tmp\n",
        "        else:\n",
        "          yh_tmp = tmp\n",
        "\n",
        "        yh[test_index] = yh_tmp[:,1]\n",
        "\n",
        "    return y, yh\n",
        "\n",
        "## KNN\n",
        "true_labels, pred_scores_knn = cross_validate(knn, x, y)\n",
        "fpr_knn, tpr_knn, _ = roc_curve(true_labels, pred_scores_knn)\n",
        "auc_knn = roc_auc_score(true_labels, pred_scores_knn)\n",
        "\n",
        "## DT\n",
        "true_labels, pred_scores_dt = cross_validate(dt, x, y)\n",
        "fpr_dt, tpr_dt, _ = roc_curve(true_labels, pred_scores_dt)\n",
        "auc_dt = roc_auc_score(true_labels, pred_scores_dt)\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "\n",
        "# plot the roc curve for the model\n",
        "plt.plot(fpr_knn, tpr_knn, marker='.', label='KNN AUC: '+str(round(auc_knn,2)))\n",
        "plt.plot(fpr_dt, tpr_dt, marker='.', label='DT AUC: '+str(round(auc_dt,2)))\n",
        "\n",
        "plt.legend()\n",
        "plt.axline((0, 0), (1, 1), linestyle=\"--\", lw=1, color='gray')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.show()"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Q0pY8u6zfgPG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Precision-Recall Curve (PRC)\n",
        "\n",
        "**Sensitivity or Recall**:\n",
        "Proportion of true positive example among actual positive (AP)\n",
        "\t\\begin{equation}\n",
        "\t\tSensitivity = Recall = \\frac{TP}{TP+FN} = \\frac{TP}{AP}\n",
        "\t\\end{equation}\n",
        "\n",
        "**Precision**:\n",
        "Proportion of true positive example among the predicted positive (PP)\n",
        "\t\\begin{equation}\n",
        "\t\tPrecision = \\frac{TP}{PP}\n",
        "\t\\end{equation}\n",
        "**F1-score**: F1 = 2 $\\times$ (precision $\\times$ recall) / (precision + recall)\n",
        "\n",
        "Precision is very important in many circumstances, e.g.,\n",
        "* We can only afford testing 5 drugs among 100 predicted drugs\n",
        "* We can admit a small number of high-risk patients among all patients\n",
        "\n",
        "These examples have the hallmark of **imbalanced** data."
      ],
      "metadata": {
        "id": "uhax3-GY4pwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data, classes = datasets.load_iris(return_X_y=True, as_frame=True)\n",
        "\n",
        "## combine Setosa and Versicolor into a single flower species\n",
        "classes[classes == 1] = 0  # comment to keep original classes, leave uncommented to fuse classes and create imbalance\n",
        "\n",
        "x = data.iloc[:,:2]\n",
        "\n",
        "class_index_to_name = {\n",
        "    0: 'Setosa',\n",
        "    1: 'Versicolor',\n",
        "    2: 'Virginica'\n",
        "}\n",
        "\n",
        "class_names = classes.map(class_index_to_name)\n",
        "\n",
        "# Get the first three colors from the 'viridis' colormap\n",
        "viridis_colors = plt.get_cmap('viridis').colors\n",
        "low_value_color = viridis_colors[0]\n",
        "middle_value_color = viridis_colors[len(viridis_colors) // 2]\n",
        "high_value_color = viridis_colors[-1]\n",
        "\n",
        "# Convert the colors to RGBA format\n",
        "low_value_rgba = mcolors.to_rgba(low_value_color)\n",
        "middle_value_rgba = mcolors.to_rgba(middle_value_color)\n",
        "high_value_rgba = mcolors.to_rgba(high_value_color)\n",
        "\n",
        "class_palette = {'Setosa': low_value_rgba, 'Versicolor': middle_value_rgba, 'Virginica': high_value_color}\n",
        "\n",
        "sns.scatterplot(data=x, x='sepal length (cm)', y='sepal width (cm)', hue=class_names, palette=class_palette)\n",
        "plt.legend(title='classes (flower species)')\n",
        "\n",
        "x = x.values\n",
        "y = classes.values\n",
        "\n",
        "## ensure that classes labelled as 0/1\n",
        "y[y == y.min()] = 0\n",
        "y[y == y.max()] = 1"
      ],
      "metadata": {
        "id": "Rq-e9YyC4tIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## create training and testing datasets. then, from the training set, create training set and validation set\n",
        "x_train, x_test, y_train, y_test = \\\n",
        "    train_test_split(x, y, test_size=0.3, random_state=42)\n",
        "\n",
        "x_train, x_valid, y_train, y_valid = \\\n",
        "    train_test_split(x_train, y_train, test_size=0.3, random_state=42)\n",
        "\n",
        "## train models\n",
        "knn = KNN(K=3)\n",
        "dt  = DecisionTree(max_depth=2)\n",
        "\n",
        "y_test_prob_knn, _ = knn.fit(x_train, y_train).predict(x_test)\n",
        "y_test_prob_dt     = dt.fit(x_train, y_train).predict(x_test)\n",
        "\n",
        "y_test_pred_knn = y_test_prob_knn.argmax(-1)\n",
        "y_test_pred_dt  = y_test_prob_dt.argmax(-1)\n",
        "\n",
        "\n",
        "### confusion matrix ###\n",
        "\n",
        "cm_knn = confusion_matrix(y_test, y_test_pred_knn)\n",
        "cm_dt  = confusion_matrix(y_test, y_test_pred_dt)\n",
        "\n",
        "cm_df_knn = pd.DataFrame(cm_knn, index=['Actual Negative','Actual Positive'],\n",
        "                     columns=['Predicted Negative', 'Predicted Positive'])\n",
        "\n",
        "cm_df_dt = pd.DataFrame(cm_dt, index=['Actual Negative','Actual Positive'],\n",
        "                     columns=['Predicted Negative', 'Predicted Positive'])\n",
        "\n",
        "print('KNN confusion matrix')\n",
        "print(cm_df_knn)\n",
        "print()\n",
        "print('DT confusion matrix')\n",
        "print(cm_df_dt)"
      ],
      "metadata": {
        "id": "9WJKa3Ox73Oh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## KNN\n",
        "true_labels, pred_scores_knn = cross_validate(knn, x, y)\n",
        "precision_knn, recall_knn, thresholds_knn = precision_recall_curve(true_labels, pred_scores_knn)\n",
        "auprc_knn = auc(recall_knn, precision_knn)\n",
        "\n",
        "## DT\n",
        "true_labels, pred_scores_dt = cross_validate(dt, x, y)\n",
        "precision_dt, recall_dt, thresholds_dt = precision_recall_curve(true_labels, pred_scores_dt)\n",
        "auprc_dt = auc(recall_dt, precision_dt)\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "\n",
        "# plot the roc curve for the model\n",
        "plt.plot(recall_knn, precision_knn, marker='.', label='KNN AUPRC: '+str(round(auprc_knn,2)))\n",
        "plt.plot(recall_dt, precision_dt, marker='.', label='DT AUPRC: '+str(round(auprc_dt,2)))\n",
        "\n",
        "plt.legend()\n",
        "plt.axline((0, 0), (1, 1), linestyle=\"--\", lw=1, color='gray')\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title('Precision-Recall curves')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lx18JZIW8Caw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "-XD88mE9tfpj",
        "arnLKjR5laRx"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}